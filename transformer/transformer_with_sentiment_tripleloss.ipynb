{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "6e9c2e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0b2c1b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\czhao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sentence_transformers import evaluation\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "import ast\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import nltk\n",
    "import re\n",
    "nltk.download('words')\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "def stemSentence_porter(sentence):\n",
    "    porter = PorterStemmer()\n",
    "    token_words=word_tokenize(sentence)\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(porter.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n",
    "\n",
    "def convert(text):\n",
    "    L = []\n",
    "    for i in ast.literal_eval(text):\n",
    "        L.append(i['name']) \n",
    "    return L \n",
    "\n",
    "\n",
    "def fetch_director(text):\n",
    "    L = []\n",
    "    for i in ast.literal_eval(text):\n",
    "        if i['job'] == 'Director':\n",
    "            L.append(i['name'])\n",
    "    return L \n",
    "\n",
    "\n",
    "def collapse(L):\n",
    "    L1 = []\n",
    "    for i in L:\n",
    "        L1.append(i.replace(\" \",\"\"))\n",
    "    return L1\n",
    "\n",
    "def clean_text(text):\n",
    "    # remove backslash-apostrophe \n",
    "    text = re.sub(\"\\'\", \"\", text) \n",
    "    # remove everything except alphabets \n",
    "    text = re.sub(\"[^a-zA-Z0-9,.’]\",\" \",text) \n",
    "    # remove whitespaces \n",
    "    text = ' '.join(text.split()) \n",
    "    # convert text to lowercase \n",
    "    \n",
    "    return text\n",
    "\n",
    "def get_cosine_sim(model, df):\n",
    "    scores = []\n",
    "    for row in zip(df['tags'], df['reviews']):\n",
    "        scores.append(util.cos_sim(model.encode(row[0]), model.encode(row[1])))\n",
    "    return scores\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "51f0e440",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv('../tmdb_5000_data/tmdb_5000_movies.csv')\n",
    "credits = pd.read_csv('../tmdb_5000_data/tmdb_5000_credits.csv') \n",
    "movies = movies.merge(credits,on='title')\n",
    "\n",
    "reviews = pd.read_csv('../crawled_data/2022-11-19_movie_info_with_reviews.csv')\n",
    "reviews = reviews[['id','reviews']]\n",
    "reviews['reviews'] = reviews['reviews'].apply(lambda x: list(map(clean_text, x.split(\"\\',\"))))\n",
    "movies = movies.merge(reviews,on='id', how='left')\n",
    "\n",
    "movies.dropna(inplace=True)\n",
    "movies['release_year'] = movies.release_date.apply(lambda x: x.split(\"-\")[0]).astype(int)\n",
    "movies = movies[movies['release_year']>1970]\n",
    "\n",
    "movies = movies[['movie_id','title','release_year','overview','genres','keywords','cast','crew','reviews']]\n",
    "movies.dropna(inplace=True)\n",
    "\n",
    "movies['genres'] = movies['genres'].apply(convert)\n",
    "\n",
    "movies['keywords'] = movies['keywords'].apply(convert)\n",
    "\n",
    "movies['cast'] = movies['cast'].apply(convert)\n",
    "\n",
    "movies['cast'] = movies['cast'].apply(lambda x:x[0:3])\n",
    "\n",
    "movies['crew'] = movies['crew'].apply(fetch_director)\n",
    "\n",
    "\n",
    "movies['crew'] = movies['crew'].apply(lambda x: [\" \".join(\\\n",
    "        w for w in nltk.wordpunct_tokenize(i) \\\n",
    "         if w.lower() in words or not w.isalpha()) for i in x])\n",
    "\n",
    "# movies['cast'] = movies['cast'].apply(collapse)\n",
    "# movies['crew'] = movies['crew'].apply(collapse)\n",
    "# movies['genres'] = movies['genres'].apply(collapse)\n",
    "# movies['keywords'] = movies['keywords'].apply(collapse)\n",
    "\n",
    "\n",
    "# new['tags'] = new['tags'].apply(stemSentence_porter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5c9eadbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['overview'] = movies['overview'].apply(lambda x:x.split())\n",
    "movies['release_year'] = movies['release_year'].astype(str).apply(lambda x:x.split())\n",
    "movies['tags'] = movies['overview'] + movies['genres'] + movies['release_year'] + movies['keywords'] + movies['cast'] + movies['crew'] \n",
    "new = movies.drop(columns=['overview','genres','keywords','cast','crew','release_year'])\n",
    "new['tags'] = new['tags'].apply(lambda x: \" \".join(x))\n",
    "new = new[(new['tags'].notnull())].reset_index(drop=True)\n",
    "new = new[new[\"reviews\"].str.len() != 0].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e1162e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "new = new.explode(['reviews'])\n",
    "new = new[new['reviews']!=''].reset_index(drop=True)\n",
    "# new.to_csv(\"../tmdb_5000_data/Cleaned_Filtered_Plots_Reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9f2836fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\czhao\\anaconda3\\lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1112: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-imdb-sentiment\")\n",
    "\n",
    "model = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-base-finetuned-imdb-sentiment\")\n",
    "\n",
    "def get_sentiment(text):\n",
    "  input_ids = tokenizer.encode(text + '</s>', return_tensors='pt')\n",
    "\n",
    "  output = model.generate(input_ids=input_ids,\n",
    "               max_length=2)\n",
    "  \n",
    "  dec = [tokenizer.decode(ids) for ids in output]\n",
    "  label = dec[0]\n",
    "  return label\n",
    "  \n",
    "a = get_sentiment(\"I like a lot that film\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "66209f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "new['review_sentiment'] = new['reviews'].apply(lambda x: get_sentiment(x)[6:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "16a705c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\czhao\\AppData\\Local\\Temp\\ipykernel_14076\\436579527.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new['sentiment_score'] = new['review_sentiment'].map({'positive': 1, 'negative': 0})\n"
     ]
    }
   ],
   "source": [
    "new['sentiment_score'] = new['review_sentiment'].map({'positive': 1, 'negative': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "84449777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>title</th>\n",
       "      <th>reviews</th>\n",
       "      <th>tags</th>\n",
       "      <th>review_sentiment</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19995</td>\n",
       "      <td>Avatar</td>\n",
       "      <td>Cameron’s epic can still thrill the audience w...</td>\n",
       "      <td>In the 22nd century, a paraplegic Marine is di...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19995</td>\n",
       "      <td>Avatar</td>\n",
       "      <td>Avatar still elicits much of the same wide eye...</td>\n",
       "      <td>In the 22nd century, a paraplegic Marine is di...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19995</td>\n",
       "      <td>Avatar</td>\n",
       "      <td>The emotional stakes presented in the final ba...</td>\n",
       "      <td>In the 22nd century, a paraplegic Marine is di...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19995</td>\n",
       "      <td>Avatar</td>\n",
       "      <td>Thirteen years after its release, Avatar still...</td>\n",
       "      <td>In the 22nd century, a paraplegic Marine is di...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19995</td>\n",
       "      <td>Avatar</td>\n",
       "      <td>A meaningful blockbuster that fails to play ig...</td>\n",
       "      <td>In the 22nd century, a paraplegic Marine is di...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1482</th>\n",
       "      <td>126186</td>\n",
       "      <td>Shanghai Calling</td>\n",
       "      <td>Shanghai Calling doesnt aspire to fresh insigh...</td>\n",
       "      <td>When ambitious New York attorney Sam is sent t...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1482</th>\n",
       "      <td>126186</td>\n",
       "      <td>Shanghai Calling</td>\n",
       "      <td>If you prefer your social commentary in the fo...</td>\n",
       "      <td>When ambitious New York attorney Sam is sent t...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1482</th>\n",
       "      <td>126186</td>\n",
       "      <td>Shanghai Calling</td>\n",
       "      <td>Shanghai Calling eventually reveals itself to ...</td>\n",
       "      <td>When ambitious New York attorney Sam is sent t...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1482</th>\n",
       "      <td>126186</td>\n",
       "      <td>Shanghai Calling</td>\n",
       "      <td>Through it all, Henney is an appealing screen ...</td>\n",
       "      <td>When ambitious New York attorney Sam is sent t...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1482</th>\n",
       "      <td>126186</td>\n",
       "      <td>Shanghai Calling</td>\n",
       "      <td>Light mainstream feature has enough charisma a...</td>\n",
       "      <td>When ambitious New York attorney Sam is sent t...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10901 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      movie_id             title  \\\n",
       "0        19995            Avatar   \n",
       "0        19995            Avatar   \n",
       "0        19995            Avatar   \n",
       "0        19995            Avatar   \n",
       "0        19995            Avatar   \n",
       "...        ...               ...   \n",
       "1482    126186  Shanghai Calling   \n",
       "1482    126186  Shanghai Calling   \n",
       "1482    126186  Shanghai Calling   \n",
       "1482    126186  Shanghai Calling   \n",
       "1482    126186  Shanghai Calling   \n",
       "\n",
       "                                                reviews  \\\n",
       "0     Cameron’s epic can still thrill the audience w...   \n",
       "0     Avatar still elicits much of the same wide eye...   \n",
       "0     The emotional stakes presented in the final ba...   \n",
       "0     Thirteen years after its release, Avatar still...   \n",
       "0     A meaningful blockbuster that fails to play ig...   \n",
       "...                                                 ...   \n",
       "1482  Shanghai Calling doesnt aspire to fresh insigh...   \n",
       "1482  If you prefer your social commentary in the fo...   \n",
       "1482  Shanghai Calling eventually reveals itself to ...   \n",
       "1482  Through it all, Henney is an appealing screen ...   \n",
       "1482  Light mainstream feature has enough charisma a...   \n",
       "\n",
       "                                                   tags review_sentiment  \\\n",
       "0     In the 22nd century, a paraplegic Marine is di...         positive   \n",
       "0     In the 22nd century, a paraplegic Marine is di...         positive   \n",
       "0     In the 22nd century, a paraplegic Marine is di...         positive   \n",
       "0     In the 22nd century, a paraplegic Marine is di...         positive   \n",
       "0     In the 22nd century, a paraplegic Marine is di...         positive   \n",
       "...                                                 ...              ...   \n",
       "1482  When ambitious New York attorney Sam is sent t...         positive   \n",
       "1482  When ambitious New York attorney Sam is sent t...         negative   \n",
       "1482  When ambitious New York attorney Sam is sent t...         negative   \n",
       "1482  When ambitious New York attorney Sam is sent t...         negative   \n",
       "1482  When ambitious New York attorney Sam is sent t...         positive   \n",
       "\n",
       "      sentiment_score  \n",
       "0                 1.0  \n",
       "0                 1.0  \n",
       "0                 1.0  \n",
       "0                 1.0  \n",
       "0                 1.0  \n",
       "...               ...  \n",
       "1482              1.0  \n",
       "1482              0.0  \n",
       "1482              0.0  \n",
       "1482              0.0  \n",
       "1482              1.0  \n",
       "\n",
       "[10901 rows x 6 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d8119400",
   "metadata": {},
   "outputs": [],
   "source": [
    "new.to_csv(\"../tmdb_5000_data/tmdb_sentiment_reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6f134dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
    "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "# model = SentenceTransformer('all-mpnet-base-v2')\n",
    "# model = SentenceTransformer('stsb-bert-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c7e64bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(model, df, batch_size=128, learning_rate=0.1, epochs=30):\n",
    "\n",
    "#     model.to(device)\n",
    "#     model.train()\n",
    "#     criterion = nn.MSELoss()\n",
    "#     optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#     plots = df['tags'].values\n",
    "    \n",
    "#     reviews = df['reviews'].values\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "#         for i in range(0, len(plots), batch_size):\n",
    "\n",
    "#             batch_plots = plots[i:i+batch_size]\n",
    "#             batch_reviews = reviews[i:i+batch_size]\n",
    "            \n",
    "#             embedding_plots = torch.tensor(model.encode(batch_plots), requires_grad=True).unsqueeze(1).to(device)\n",
    "#             embedding_reviews = torch.tensor(model.encode(batch_reviews), requires_grad=True).unsqueeze(1).to(device)\n",
    "            \n",
    "#             loss = criterion(embedding_plots, embedding_reviews)\n",
    "#             print(loss)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfb1f164",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train(model, new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c8710d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# descriptions = new['tags'].tolist()\n",
    "# des_embeddings = []\n",
    "# for i,des in enumerate(descriptions):\n",
    "#     des_embeddings.append(model.encode(des))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01479e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def recommend(query, model):\n",
    "#     #Compute cosine-similarities with all embeddings \n",
    "#     model.eval()\n",
    "#     query_embedd = model.encode(query)\n",
    "#     cosine_scores = util.pytorch_cos_sim(query_embedd, des_embeddings)\n",
    "#     top5_matches = torch.argsort(cosine_scores, dim=-1, descending=True).tolist()[0][0:5]\n",
    "#     return top5_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfdf0b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_show_des = 'moon Pandora space colony society Sam 3d'\n",
    "# recommendded_results = recommend(query_show_des, model)\n",
    "\n",
    "# for index in recommendded_results:\n",
    "#     print(new.iloc[index,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cbf6702a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import models, losses, InputExample\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8341a9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/mpnet-base were not used when initializing MPNetModel: ['lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing MPNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MPNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MPNetModel were not initialized from the model checkpoint at microsoft/mpnet-base and are newly initialized: ['mpnet.pooler.dense.weight', 'mpnet.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# microsoft/mpnet-base\n",
    "# sentence-transformers/stsb-bert-large\n",
    "word_embedding_model = models.Transformer('microsoft/mpnet-base').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3562c1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), \n",
    "                                  pooling_mode_mean_tokens=True, \n",
    "                                  pooling_mode_cls_token=False, \n",
    "                                  pooling_mode_max_tokens=False\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7908a249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the model\n",
    "sent_model = SentenceTransformer(modules=[word_embedding_model, pooling_model]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f47d6c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = CrossEncoder('cross-encoder/stsb-roberta-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8ddd1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = model.predict(new[['tags','reviews']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf749371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f701ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = []\n",
    "\n",
    "for index, row in enumerate(zip(new['tags'], new['reviews'])):\n",
    "#     train_examples.append(InputExample(texts=[row[0], row[1]], label = [scores[index]]))\n",
    "    train_examples.append(InputExample(texts=[row[0], row[1]]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63f1d288",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=2)\n",
    "# MultipleNegativesRankingLoss not trainable because gpu limitation\n",
    "# MegaBatchMarginLoss \n",
    "# MSELoss need labels\n",
    "train_loss = losses.MegaBatchMarginLoss(sent_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "720899d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev_mse = evaluation.EmbeddingSimilarityEvaluator(new['tags'].values, new['reviews'].values,\\\n",
    "#                                                   scores=scores)\n",
    "dev_mse = evaluation.MSEEvaluator(new['tags'].values, new['reviews'].values, teacher_model=sent_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b147a3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev_mse = evaluation.MSEEvaluator(new['tags'].values, new['reviews'].values,teacher_model=sent_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "804f0b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb75de56607c4d4fbadce1112625448a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f18e6118e6b490e8816c56ea8a7b067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/474 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "806ef433facb4963b310ea6c96fa00da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/474 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9aa0c7998f9408dbffdca65e6b9d792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/474 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sent_model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=3, warmup_steps=100, evaluator=dev_mse, output_path='result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "35bdd903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores_after_training = get_cosine_sim(sent_model, new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "21691c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum(scores_before_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f81110c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum(scores_after_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "544591e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = new['tags'].tolist()\n",
    "des_embeddings = []\n",
    "for i,des in enumerate(descriptions):\n",
    "    des_embeddings.append(sent_model.encode(des))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9f0d160f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(query, model):\n",
    "    #Compute cosine-similarities with all embeddings \n",
    "    model.eval()\n",
    "    query_embedd = model.encode(query)\n",
    "    cosine_scores = util.pytorch_cos_sim(query_embedd, des_embeddings)\n",
    "    top5_matches = torch.argsort(cosine_scores, dim=-1, descending=True).tolist()[0][0:5]\n",
    "    return top5_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "407e04df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie_id                                                77951\n",
      "title                                  Walking With Dinosaurs\n",
      "reviews     Top notch education and entertainment for dino...\n",
      "tags        Walking with Dinosaurs 3D is a film depicting ...\n",
      "Name: 170, dtype: object\n",
      "movie_id                                                15512\n",
      "title                                      Monsters vs Aliens\n",
      "reviews     Wonderfully inventive characters and highly po...\n",
      "tags        When Susan Murphy is unwittingly clobbered by ...\n",
      "Name: 28, dtype: object\n",
      "movie_id                                                 9297\n",
      "title                                           Monster House\n",
      "reviews     here the setting is unambiguously autumnal, in...\n",
      "tags        Monsters under the bed are scary enough, but w...\n",
      "Name: 183, dtype: object\n",
      "movie_id                                                50321\n",
      "title                                         Mars Needs Moms\n",
      "reviews     A decent flick that could have been so much mo...\n",
      "tags        When Martians suddenly abduct his mom, mischie...\n",
      "Name: 62, dtype: object\n",
      "movie_id                                                10386\n",
      "title                                          The Iron Giant\n",
      "reviews     The characters are far more complex than most ...\n",
      "tags        In the small town of Rockwell, Maine in Octobe...\n",
      "Name: 296, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\czhao\\anaconda3\\lib\\site-packages\\sentence_transformers\\util.py:39: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:204.)\n",
      "  b = torch.tensor(b)\n"
     ]
    }
   ],
   "source": [
    "query_show_des = '3D Top notch education and entertainment for dinosaurs'\n",
    "recommendded_results = recommend(query_show_des, sent_model)\n",
    "\n",
    "for index in recommendded_results:\n",
    "    print(new.iloc[index,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e8710529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie_id                                                 7278\n",
      "title                                       Meet the Spartans\n",
      "reviews     The worlds worst parody movie. , This is one o...\n",
      "tags        From the creators of Scary Movie and Date Movi...\n",
      "Name: 712, dtype: object\n",
      "movie_id                                                13805\n",
      "title                                          Disaster Movie\n",
      "reviews     Nothing more than a 90 minute barrage of unfun...\n",
      "tags        In DISASTER MOVIE, the filmmaking team behind ...\n",
      "Name: 886, dtype: object\n",
      "movie_id                                                82690\n",
      "title                                          Wreck-It Ralph\n",
      "reviews     Phil Johnston and Jennifer Lee’s script finds ...\n",
      "tags        Wreck-It Ralph is the 9-foot-tall, 643-pound v...\n",
      "Name: 72, dtype: object\n",
      "movie_id                                                93456\n",
      "title                                         Despicable Me 2\n",
      "reviews     Nothing is surprising about Despicable Me 2, b...\n",
      "tags        Gru is recruited by the Anti-Villain League to...\n",
      "Name: 300, dtype: object\n",
      "movie_id                                                77951\n",
      "title                                  Walking With Dinosaurs\n",
      "reviews     Top notch education and entertainment for dino...\n",
      "tags        Walking with Dinosaurs 3D is a film depicting ...\n",
      "Name: 289, dtype: object\n"
     ]
    }
   ],
   "source": [
    "query_show_des = 'Its Godzilla versus the paper pushers'\n",
    "recommendded_results = recommend(query_show_des, sent_model)\n",
    "\n",
    "for index in recommendded_results:\n",
    "    print(new.iloc[index,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e09cb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5fa680df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 97% |\n",
      "GPU Usage after emptying the cache\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  2% |  5% |\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from numba import cuda\n",
    "\n",
    "def free_gpu_cache():\n",
    "    print(\"Initial GPU Usage\")\n",
    "    gpu_usage()                             \n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "    cuda.select_device(0)\n",
    "\n",
    "    print(\"GPU Usage after emptying the cache\")\n",
    "    gpu_usage()\n",
    "\n",
    "free_gpu_cache()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7eeb4e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d282d907",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
